---
title: "pHacking_tutorial"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{pHacking_tutorial}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup, echo=F}
library(easypackages)
packages <- c('tidyverse', 'MASS', 'devtools', 'parallel', 'pbmcapply', 'data.table')
libraries(packages)
load_all()

num_dvs_data <- readRDS(file = 'data/num_dvs_data.RData')
```


```{r comparison_to_stefan_2022, echo=F}
sim_params <- list('step_size' = c(1, 5, 10),
                   'n_min' = c(1), 
                   'n_max' = c(30), #, 100, 300), 
                   'num_dvs' = 1, 
                   'num_ivs' = 1, 
                   'dv_cor' = 0,
                   'iv_cor' = 0)

num_iterations <- 1e3
sim_data <- run_exp_simulations(sim_params = sim_params, num_iterations = 1, 
                                num_cores = detectCores() - 1, seed = 27, step_size = T)


sim_data$false_pos_exist <- unlist(lapply(X = 1:nrow(sim_data), FUN = false_pos_exist, sim_data = sim_data))

sim_data %>% 
  group_by(n_min, step_size, iv_cor) %>%
  summarize(false_pos_rate = sum(false_pos_exist)/num_iterations)

sim_data %>% 
  group_by(n_min, num_dvs, dv_cor) %>%
  summarize(false_pos_rate = sum(false_pos_exist)/num_iterations)
```


```{r cor_investigation, echo=F}
#using code from stefan, correlations are only set between the DVs, but the correlations between the levels of the DVs (i.e. ,groups) are 
#not explicitly set and are a byproduct of the DV correlations 

#nvar: Number of dependent variables in the data frame
#nobs.group: Vector giving number of observations per group
# nobs: Number of observations (rows) in the simulated data frame
#r: Desired correlation between the dependent variables (scalar)


nobs.group <- c(1e3, 1e3)
nvar <- 3
r <- .30

stefan_data <- gen_stefan_data(npbs.group = nobs.group, nvar = nvar, r = r)

lapply(X = 2:4, FUN = compute_control_test_cor, stefan_data = stefan_data)
  
r <- .30
nobs <- 

.sim.multcor <- function(nobs, nvar, r, mu = 0, sd = 1, missing = 0){

  # set up correlation matrix
  R <- matrix(rep(r, nvar**2), nrow = nvar)
  diag(R) <- rep(1, nvar)

  # transposed Cholesky decomposition of correlation matrix
  U <- t(chol(R))

  # create random noise matrix
  random.normal <- matrix(stats::rnorm(nvar*nobs, mu, sd), nrow=nvar, ncol=nobs)

  # create raw data from matrix multiplication of U and random noise
  X <- as.data.frame(t(U %*% random.normal))
}
```

```{r echo=F}
##Example: Consider a developmental researcher with an archival data base containing data for several dependent and independent variables. For example,
##data on shyness, anxiety, self-reported stress, happiness exists for children. Among these data, the researcher can group the data in several ways:
#sex, race, SES, attachment style, etc. Thus, there is an ample opportunity to conduct pairwise comparisons in an uncontrolled manner.

#Factors to simulate:
##1) Cell size: 30, 50, 100, 300, 500
##2) Selective reporting with respect to the IV (3, 5, 10 groups). Each group has two levels and a t test is computed between the two levels
##(i.e., control vs test). A comparison is reported only if it is significant.
##3) Selective reporting with respect to the DV (3, 5, 10 variables). Data for each DV exists for each group. Thus, if there are 3 groups and 3 DVs,
##then data for each DV provides data for 3 groups (i.e., 3 t-tests), meaning a total of 9 t-tests are computed.
##4) Correlation among DVs (0.16; average correlation in I-O psychology).
##5) Correlation among IV levels (0.16; average correlation in I-O psychology). Correlation between IVs would zero, otherwise they would cease to be independent.
##for the desired number of variables.
##6) Optional stopping (N_add = 5, 10, 50) with Nmax = {30, 50, 100, 300, 500} 
##7) Variable transformations (logarithmic, reciprocal, square root transformation) on DV, predictor, or both. 
##8) Missing data (MCAR or MNAR with percentages of 10%, 30%, and 50% (see Peugh & Enders, 2004). 


## Additional conditions to simulate under
##1) Missing data (missing not at random: the probability that a value is missing depends on the missing data value themselves) For example, probability of
##response decreases with value on variable. Manipulate percentages to be 10%, 30%, and 50% (see Peugh & Enders, 2004). Mean percentage of missing data was 7.60%, with average of
##7.09% in cross-sectional designs and 9.78% in longitudinal designs.
##2) Measurement error

##Measurement of false positive rate.
##First, understand that the false positive rate is a measure of incidence in a family of tests. A family of tests is determined by the factors that
##manipulated. For example, if there are 3 groups and three DVs, then a family of tests would contains 9 t tests. If any of these tests returns
##significant, then the
#False positive rate: proportion of

##P-value selection method
##1) First significant
##2) Smallest p-value

#Optional stopping
##Simulated by adding the desired number of data points and repeating the transaction. The new list of p_values are added to the current one. The 
##procedure is repeated until the maximum sample is obtained. Optional stopping will be manipulated by using a more realistic manipulation than used in 
## Stefan et al. (2022). Nmax = {30, 50, 100, 250, 500} and step size = {10, 25, or 50}. 

#Covariates
##Simulated by computing bivariate regression and then adding covariates one at a time. Number of covariates = {3, 5, 10}. Correlation of covariates is .16 or .30. 

#Transformations
###Simulated by using logarithmic, reciprocal, and square root transformations on DV, predictor, or both. 

#Discretizing variables
##Simulated by conducting median split on continuous variable or cut-the-middle split. 

#Missing data
##Simulated by having data be MCAR or MNAR for 10, 30, or 50% of the data. 

#for each sample size increment, generate a data set with that many data points ```
#list containing results from each iteration
```

```{r iv_code, echo=F}

#nobs.group Scalar defining number of observations per group
#' @param nvar Number of independent variables in the data frame
#' @param r Desired correlation between the independent variables (scalar)
nvar <- 2
r <- .30
nobs.group <- 100

  nobs.group <- rep(nobs.group, 2)
  # Simulate control group
  control <- rnorm(nobs.group[1])

  # Simulate multiple experimental groups
  #ivs <- .sim.multcor(nobs = nobs.group[2], nvar = nvar, r = r)

  nobs <-  nobs.group[2]
 
  # set up correlation matrix
  R <- matrix(rep(r, nvar**2), nrow = nvar)
  diag(R) <- rep(1, nvar)

  # transposed Cholesky decomposition of correlation matrix
  U <- t(chol(R))

  # create random noise matrix
  random.normal <- matrix(stats::rnorm(nvar*nobs, 0, 1), nrow=nvar, ncol=nobs)

  # create raw data from matrix multiplication of U and random noise
  ivs <- as.data.frame(t(U %*% random.normal))
  
   df <- cbind(control, ivs)
  
#' @param df Data frame (wide format) containing a control group variable and multiple treatment group variables
#' @param ivs Location of the independent variables (treatment groups) in the (wide) data frame
#' @param control Location of the control group in the (wide) data frame
#' @param strategy String value: One out of "firstsig", "smallest", "smallest.sig"
#' @param alternative Direction of the t-test ("two.sided", "less", "greater")
#' @param alpha Significance level of the t-test (default: 0.05)
#' @importFrom stats t.test

  multIVhack <- function(df, ivs, control, strategy = "firstsig", alternative = "two.sided", alpha = 0.05){

  treatm <- df[, 2:4]
  control <- df[, 1]

  # Prepare dataset
  mod <- list()
  r2s <- rep(NA, length(ivs))

  # Compute t-tests
  for(i in 1:length(ivs)){
    mod[[i]] <- stats::t.test(control, treatm[,i], var.equal = TRUE, alternative = alternative)
    r2s[i] <- .compR2t(control, treatm[,i])
  }

```

